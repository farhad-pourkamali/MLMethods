{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133a4b10",
   "metadata": {},
   "source": [
    "# Data Preparation and Validation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cde38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Methods \n",
    "\n",
    "\n",
    "### Module 2: Data Preparation and Validation Techniques\n",
    "\n",
    "### Instructor: Farhad Pourkamali\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1b2ba",
   "metadata": {},
   "source": [
    "### Overview \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "1. Data Loading, Understanding, and Filtering (https://youtu.be/FvlAINfSLyg)\n",
    "    * Understanding data structure (input features, target variables, and types of data)\n",
    "    * Descriptive statistics \n",
    "    * Data visualization for understanding distributions and relationships (e.g., histograms, scatter plots, correlation matrices)\n",
    "2. Feature Engineering (https://youtu.be/CFA877KhPl0)\n",
    "    * Feature scaling (standardization, normalization)\n",
    "    * Feature transformation (e.g., polynomial features)\n",
    "3. Data Splitting and Cross-Validation (https://youtu.be/brqCqB3xfX8)\n",
    "    * Train-Test Split \n",
    "    * Validation Set (Role of a validation set in model tuning)\n",
    "    * K-Fold Cross-Validation (Description and implementation)\n",
    "    * Leave-One-Out Cross-Validation (LOOCV)\n",
    "5. Pipelines for Automated Preprocessing (https://youtu.be/1sm0ZcVZK8c)\n",
    "    * Integrating feature engineering and predictive modeling in `sklearn` pipelines\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e2605",
   "metadata": {},
   "source": [
    "# 1. Data Loading, Understanding, and Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d03e8b",
   "metadata": {},
   "source": [
    "* `pandas.read_csv` is a function to load CSV (Comma Separated Values) files into a Pandas DataFrame for analysis and manipulation.\n",
    "\n",
    "* Hereâ€™s the general syntax for the `pandas.read_csv` function (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):\n",
    "\n",
    "```Python \n",
    "pandas.read_csv(\n",
    "    filepath_or_buffer,        # Filepath or URL to the CSV file\n",
    "    sep=',',                   # Delimiter (default is ',')\n",
    "    header='infer',            # Row number to use as header (default is the first row)\n",
    "    index_col=None,            # Column(s) to use as the index\n",
    "    usecols=None,              # Columns to read (list of column names or indices)\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/farhad-pourkamali/MATH4388Online/refs/heads/main/data/Auto.csv\")\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea858b",
   "metadata": {},
   "source": [
    "* The **index rows** of a DataFrame can be accessed using the `.index` attribute\n",
    "\n",
    "* The **column names** of a DataFrame can be accessed using the `.columns` attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the index\n",
    "print(\"Index Rows:\")\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23146c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "print(\"Column Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685162d",
   "metadata": {},
   "source": [
    "* Large datasets can have thousands of rows and columns.\n",
    "\n",
    "* Displaying the entire DataFrame (`df`) floods the output, making it hard to read or find relevant information.\n",
    "\n",
    "* `df.head()` returns only the first 5 rows by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53c98d",
   "metadata": {},
   "source": [
    "* The `.info()` method in Pandas is used to provide a concise summary of a DataFrame, giving key information about the structure and contents of the data. It is especially useful for getting an overview of large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Do you see any problems? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"horsepower\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"horsepower\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a column with str values to floats\n",
    "# Convert non-numeric values (e.g., 'abc', '?', 'N/A') to NaN\n",
    "\n",
    "df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc927fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae05101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values\n",
    "\n",
    "print(df['horsepower'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22780a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with at least one NaN\n",
    "\n",
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7022764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column in-place\n",
    "df_cleaned.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d3de8",
   "metadata": {},
   "source": [
    "* The `.describe()` method provides a statistical summary of numerical columns in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# customization \n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# histogram\n",
    "df_cleaned.hist(bins=10, figsize=(12, 8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6abf39",
   "metadata": {},
   "source": [
    "* Seaborn (https://seaborn.pydata.org/index.html) is a Python library built on top of Matplotlib, designed to simplify the creation of complex and attractive statistical plots with minimal code.\n",
    "\n",
    "* Seaborn works seamlessly with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df_cleaned, diag_kind='hist', corner=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f7e0a",
   "metadata": {},
   "source": [
    "For two vectors $\\mathbf{x}$ and $\\mathbf{y}$ (or columns of a DataFrame), the Pearson correlation coefficient $\\rho$ is given by\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{\\mathbf{x}})(y_i - \\bar{\\mathbf{y}})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{\\mathbf{x}})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{\\mathbf{y}})^2}}\n",
    "$$\n",
    "\n",
    "* $\\rho=1$: Perfect positive linear relationship\n",
    "* $\\rho=-1$: Perfect negative linear relationship\n",
    "* $\\rho=0$: No linear relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_cleaned.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 5))  \n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5982df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_counts with normalize=True to calculate percentages\n",
    "percentages = df_cleaned['origin'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentages\n",
    "print(\"\\nPercentages of each group:\")\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_counts with normalize=True to calculate percentages\n",
    "percentages = df_cleaned['cylinders'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentages\n",
    "print(\"\\nPercentages of each group:\")\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8860a23",
   "metadata": {},
   "source": [
    "*  Next, we write a function that keeps cars with 4, 6, and 8 cylinders, selects appropriate columns for input features and output labels, and returns them as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_select_features(data):\n",
    "    \"\"\"\n",
    "    Filters the dataset for cars with 4, 6, and 8 cylinders, \n",
    "    selects input features and output labels, and returns them as DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input DataFrame containing car data.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): DataFrame containing input features.\n",
    "        y (pd.DataFrame): DataFrame containing the output labels (mpg).\n",
    "    \"\"\"\n",
    "    # Step 1: Filter rows for cars with 4, 6, and 8 cylinders\n",
    "    filtered_data = data[data['cylinders'].isin([4, 6, 8])]\n",
    "    \n",
    "    # Step 2: Select columns for input features \n",
    "    input_columns = [\"horsepower\", \"weight\"]\n",
    "    X = filtered_data[input_columns]\n",
    "    \n",
    "    # Step 3: Select the \"mpg\" column as the output label\n",
    "    y = filtered_data[[\"mpg\"]]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdec1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = filter_and_select_features(df_cleaned)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63673add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extract data\n",
    "x1 = X[\"horsepower\"]\n",
    "x2 = X[\"weight\"]\n",
    "x3 = y[\"mpg\"]\n",
    "\n",
    "# Create scatter plot\n",
    "ax.scatter(x1, x2, x3, c='r', marker='o')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel(\"horsepower\")\n",
    "ax.set_ylabel(\"weight\")\n",
    "ax.set_zlabel(\"mpg\")\n",
    "\n",
    "ax.view_init(elev=20, azim=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fa2b9",
   "metadata": {},
   "source": [
    "* Hence, the regression problem can be viewed as \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{weight}\n",
    "\\end{equation}\n",
    "\n",
    "* Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated\n",
    "    * Makes it difficult to assess the individual effect of each predictor on the dependent variable\n",
    "    * Reduces model interpretability and reliability\n",
    "\n",
    "* Here are several possible ways to address multicollinearity\n",
    "    * Remove one or more of the highly correlated predictors\n",
    "    * Combine correlated features into a single feature using aggregation or transformation\n",
    "    * Apply penalties to large coefficients, reducing the impact of multicollinearity\n",
    "    * Use models that handle multicollinearity well (e.g., Decision Trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ab835",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72adf339",
   "metadata": {},
   "source": [
    "* When features are in different ranges (e.g., weights and horsepowers), machine learning methods can behave poorly due to the dominance of larger-scaled features.\n",
    "\n",
    "* Techniques to address scale issues:\n",
    "    * Standardization: Rescales features to have a mean of 0 and standard deviation of 1. Useful for algorithms assuming Gaussian-like distributions or when feature magnitudes vary greatly.\n",
    "    * Normalization: Rescales features to be in the range $[0, 1]$. Useful for algorithms sensitive to magnitude but not distribution.\n",
    "\n",
    "* Equations ($\\mu$ represents the mean and $\\sigma$ is the standard deviation)\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "standardized = scaler.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(standardized, columns=X.columns)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afe7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we verify?\n",
    "\n",
    "print(\"means:\\n\", X.sum(axis=0))\n",
    "\n",
    "print(\"standard deviations:\\n\", X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923899a7",
   "metadata": {},
   "source": [
    "### Polynomial Transformation\n",
    "\n",
    "* Polynomial transformation increases the dimensionality of your feature space by adding higher-degree combinations of features.\n",
    "    * This can help capture complex, nonlinear relationships between the features and the target variable.\n",
    "\n",
    "* Given two features, $x_1$ (horsepower) and $x_2$ (weight), a polynomial transformation of degree $d$ creates new features by including all combinations of $x_1$ and $x_2$ up to the given degree.\n",
    "    * Degree 1: $x_1$ and $x_2$\n",
    "    * Degree 2: $x_1$, $x_2$, $x_1^2$, $x_2^2$, $x_1x_2$\n",
    "    \n",
    "* Note that we can include \"1\", which corresponds to the bias term (intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Polynomial transformation of degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "feature_names = poly.get_feature_names_out(input_features=X.columns)\n",
    "\n",
    "poly_X = pd.DataFrame(poly_features, columns=feature_names)\n",
    "\n",
    "poly_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d05a2",
   "metadata": {},
   "source": [
    "* Hence, the regression model works in the following form \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{weight} + \\beta_3 \\cdot \\text{horsepower}^2 + \\beta_4 \\cdot \\text{horsepower} \\cdot \\text{weight} + \\beta_5 \\cdot \\text{weight}^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97538048",
   "metadata": {},
   "source": [
    "# 3. Data Splitting and Cross-Validation\n",
    "\n",
    "* Train-Test Split: This is a crucial step in machine learning where you divide your data set into two parts:\n",
    "\n",
    "    * Training set: Used to train the machine learning model.\n",
    "    * Testing set: Used to evaluate the trained model's performance on unseen data.\n",
    "    \n",
    "    <img src=\"https://github.com/farhad-pourkamali/MATH4388Online/blob/main/images/train_test.png?raw=true\" width=350>\n",
    "    \n",
    "\n",
    "\n",
    "* Random Sampling:\n",
    "\n",
    "    * Each data point has an equal chance of being assigned to either the training or testing set.\n",
    "    * Simple and often sufficient for large, well-balanced data sets.\n",
    "\n",
    "* Using `train_test_split`:\n",
    "\n",
    "    * You can easily split your data using the `train_test_split` method from the `sklearn.model_selection` module.\n",
    "    * Specify the desired `test_size` (e.g., 0.25 for a 25% test set) and set a `random_state` (e.g., random_state=42) to ensure that you get the same split every time you run your code. \n",
    "        * Your model's performance might vary significantly depending on the specific random split generated by the chosen `random_state`.\n",
    "        * If you only evaluate with one split, you might get an overly optimistic or pessimistic estimate of how well your model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f781252",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10506f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364615e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random Sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b70867",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d39e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d513b",
   "metadata": {},
   "source": [
    "### Validation Set \n",
    "\n",
    "* Machine learning models have hyperparameters that need to be adjusted for optimal performance. For example, consider the polynomial transformation of the input features (degree 1 vs. 2)\n",
    "\n",
    "$$\n",
    "\\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{weight}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{weight} + \\beta_3 \\cdot \\text{horsepower}^2 + \\beta_4 \\cdot \\text{horsepower} \\cdot \\text{weight} + \\beta_5 \\cdot \\text{weight}^2 \n",
    "$$\n",
    "\n",
    "* If you only have train and test sets, you might end up overfitting your model to the test set during hyperparameter tuning.\n",
    "* This happens because you are repeatedly evaluating on the test set and indirectly \"learning\" its characteristics. \n",
    "\n",
    "* A separate validation or development set prevents this.\n",
    "\n",
    "<img src=\"https://github.com/farhad-pourkamali/MATH4388Online/blob/main/images/model_selection.png?raw=true\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9215630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have this:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Now, split the training data again to create a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=42  # Use a different test_size here\n",
    ")\n",
    "\n",
    "# You now have:\n",
    "# - X_train, y_train: Training set\n",
    "# - X_val, y_val: Validation set\n",
    "# - X_test, y_test: Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c866d",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "* Challenges of Train-Test Split\n",
    "\n",
    "    * Limited Data: Splitting data into three sets (train, validation, test) can significantly reduce the amount of data available for training, especially with smaller data sets.\n",
    "    * Split Dependency: Results can vary depending on the random split of training and validation data.\n",
    "* What is Cross-Validation (CV)?\n",
    "\n",
    "    * No Separate Validation Set: CV eliminates the need for a dedicated validation set.\n",
    "    * K-fold CV:\n",
    "        * Divide the data into K equal-sized folds.\n",
    "        * Train the model K times, each time using K-1 folds for training and the remaining fold for validation.\n",
    "        * Average the performance across all K folds to get a more robust performance estimate.\n",
    "        * A test set should still be held out for final evaluation, but the validation or development set is no longer needed when doing CV.\n",
    "        \n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=500>\n",
    "\n",
    "* Benefits of CV:\n",
    "\n",
    "    * More Effective Data Usage: Utilizes more data for training compared to having a separate validation set.\n",
    "    * Reduced Split Dependency: Less sensitive to how the data is split, providing a more reliable performance estimate.\n",
    "    \n",
    "* Leave-One-Out Cross-Validation (LOOCV) is a special case of K-fold cross-validation where K is equal to the number of data points in your data set.\n",
    "    * Each fold consists of a single data point.\n",
    "    \n",
    "* The `cross_val_score` function from `sklearn.model_selection` helps you evaluate your model's performance using K-fold cross-validation.\n",
    "\n",
    "* Syntax: \n",
    "```Python\n",
    "cross_val_score(estimator, X, y=None, scoring=None, cv=None)\n",
    "```\n",
    "\n",
    "* Parameters:\n",
    "\n",
    "    * estimator: The model or object to use to fit the data. (e.g., `LinearRegression()`)\n",
    "    * X: The data to fit. Can be, for example, a list, or an array.\n",
    "    * y: The target variable to try to predict. (Optional for unsupervised learning)\n",
    "    * scoring: A string or a callable to evaluate the predictions on the test set. (e.g., 'accuracy', 'f1', 'roc_auc')\n",
    "    * cv: Determines the cross-validation splitting strategy (e.g., an integer for the number of folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc25e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='r2', cv=5)  \n",
    "\n",
    "# Print the scores for each fold\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# Calculate and print the average score\n",
    "print(\"Average cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on full training data\n",
    "model = LinearRegression()  \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# Evaluate using the test data \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"final score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1350b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5) \n",
    "\n",
    "# Add the 45-degree line\n",
    "min_value = min(min(y_test.to_numpy()), min(y_pred))\n",
    "max_value = max(max(y_test.to_numpy()), max(y_pred))\n",
    "plt.plot([min_value, max_value], [min_value, max_value], 'k--', lw=2)  # Dashed line\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f4adb",
   "metadata": {},
   "source": [
    "* In the following, we will use `cross_val_score` to find the optimal degree of the polynomial transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal degree using cross-validation\n",
    "best_degree = 1  # Start with degree 1\n",
    "best_cv_score = -np.inf \n",
    "\n",
    "for degree in range(1, 11):  # Check degrees from 1 to 10\n",
    "    \n",
    "    # Create polynomial features for the current degree\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_poly, y_train, scoring='r2', cv=5)\n",
    "    \n",
    "    # Calculate the average\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "    if mean_cv_score > best_cv_score:\n",
    "        best_cv_score = mean_cv_score\n",
    "        best_degree = degree\n",
    "\n",
    "print(f\"Best degree: {best_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1da1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on full training data\n",
    "poly = PolynomialFeatures(degree=best_degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "model = LinearRegression()  \n",
    "model.fit(X_train_poly, y_train) \n",
    "\n",
    "# Evaluate using the test data \n",
    "y_pred = model.predict(poly.transform(X_test))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"final score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaae00",
   "metadata": {},
   "source": [
    "* In sklearn, `cross_validate` is more versatile than `cross_val_score`, allowing evaluation of multiple metrics, access to training scores, and collection of additional information (e.g., fit time and score time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74599f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create polynomial features for the current degree\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "    \n",
    "# Perform 5-fold cross-validation\n",
    "results = cross_validate(model, X_train_poly, y_train, scoring='r2', cv=5)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d8a1e",
   "metadata": {},
   "source": [
    "* Choose `cross_val_score` for simplicity and `cross_validate` for flexibility and more detailed insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bdd00",
   "metadata": {},
   "source": [
    "# 4. Pipelines for Automated Preprocessing\n",
    "\n",
    "* A pipeline is a way to chain together multiple data transformations and a final estimator (i.e., a machine learning model) into a single, cohesive object. \n",
    "\n",
    "<img src=\"https://github.com/farhad-pourkamali/MATH4388Online/blob/main/images/pipeline.png?raw=true\" width=600>\n",
    "\n",
    "* It simplifies your code and ensures that the same steps are applied consistently during both training and prediction.\n",
    "\n",
    "* Syntax \n",
    "```Python \n",
    "Pipeline(steps)\n",
    "``` \n",
    "\n",
    "* Parameters: \n",
    "    * steps: List of `(name, transform)` tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/farhad-pourkamali/MATH4388Online/refs/heads/main/data/Auto.csv\")\n",
    "\n",
    "df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')\n",
    "\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Select input features and outputs \n",
    "X, y = filter_and_select_features(df_cleaned)\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),           # Feature scaling\n",
    "    ('poly', PolynomialFeatures(degree=2)), # Polynomial features\n",
    "    ('model', LinearRegression())           # Linear regression model\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=20)\n",
    "\n",
    "# Fit the pipeline (all steps together)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"final score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6703b0b",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "* Chapter 2 of Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow: https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbff76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
